import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
# ----------------- Step 0: Prepare data -----------------
import dotenv
dotenv.load_dotenv()

CHROMA_PATH = "chroma_db"

FOLDER_PATH='Chinese-Laws'

# ----------------- Step 3: Embedding and storage -----------------
# Use OpenAIEmbeddings to create embedding model
print("3. Creating embeddings and storing to vector database...")
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Use Chroma as vector store
# Default storage in local .chroma_db folder
if not os.path.exists(CHROMA_PATH) or not os.listdir(CHROMA_PATH):

    print(f"ğŸ†• Database not detected, creating and storing to {CHROMA_PATH}...")
    if not os.path.exists(CHROMA_PATH):
        os.makedirs(CHROMA_PATH)
    # ----------------- Step 1: Load documents -----------------
    print("1. Loading documents...")
    loader = DirectoryLoader(
    path=FOLDER_PATH,
    glob="**/*.txt",
    loader_cls=TextLoader,
    loader_kwargs={'encoding': 'utf-8'}  
    )
    documents = loader.load()
    print(f"Loaded {len(documents)} documents")
    # ----------------- Step 2: Split documents -----------------
    print("2. Splitting documents...")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, # Maximum length of each chunk
        chunk_overlap=50, # Overlap length between chunks
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    splits = text_splitter.split_documents(documents)
    print(f"Split {len(documents)} documents into {len(splits)} chunks")
    # Execute time-consuming embedding operations here
    vectorstore = Chroma.from_documents(
        documents=splits, 
        embedding=embeddings, 
        persist_directory=CHROMA_PATH
    )
    print("âœ… Database creation completed!")
else:
    print(f"ğŸ”„ Detected existing vector database ({CHROMA_PATH}), loading directly...")
    vectorstore = Chroma(
        persist_directory=CHROMA_PATH, 
        embedding_function=embeddings
    )
# Set vector store as retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# ----------------- Step 4 & 5: Define RAG chain and query -----------------
# 4. Define prompt template for enhanced generation
prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ã€æ³•å¾‹æ³•è§„ä¸Šä¸‹æ–‡ã€‘æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. å›ç­”å¿…é¡»**åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡**ï¼Œä¸è¦ç¼–é€ æ³•å¾‹æ¡æ–‡ã€‚
2. å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´â€œæ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•å›ç­”â€ã€‚

ã€æ³•å¾‹æ³•è§„ä¸Šä¸‹æ–‡ã€‘:
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘:
{input}

ã€å›ç­”ã€‘:
""")






# ----------------- Run query -----------------
query = "èµ°ç§ç½ªæ€ä¹ˆåˆ¤åˆ‘ï¼Ÿ"

print(f"ğŸ” Retrieving: {query}")
retrieved_docs = retriever.invoke(query)
# Print results
print(f"\nâœ… Retrieved {len(retrieved_docs)} relevant document(s):\n")

# Combine documents into a single string
context_string = ""
for i, doc in enumerate(retrieved_docs):
    # Get filtered metadata (only crime_small and crime_big)
    
    
    # Build document string
    doc_string = f"--- Document {i+1} ---\n"
    doc_string += f"[Content]: {doc.page_content[:300]}\n"
    
    context_string += doc_string

print(context_string)