import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

from langchain_core.prompts import ChatPromptTemplate
# ----------------- Step 0: Prepare data -----------------
from read_rag_data import load_json_to_documents
import dotenv
dotenv.load_dotenv()
# ----------------- Step 1: Load documents -----------------
documents = load_json_to_documents()
CHROMA_PATH = "./chroma_db_legal"
# ----------------- Step 2: Split documents -----------------
# Use RecursiveCharacterTextSplitter to split documents into chunks
print("2. Splitting documents...")

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500, # Maximum length of each chunk
    chunk_overlap=50, # Overlap length between chunks
    length_function=len
)
splits = text_splitter.split_documents(documents)

# ----------------- Step 3: Embedding and storage -----------------
# Use OpenAIEmbeddings to create embedding model
print("3. Creating embeddings and storing to vector database...")
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Use Chroma as vector store
# Default storage in local .chroma_db folder
if os.path.exists(CHROMA_PATH) and os.listdir(CHROMA_PATH):
    print(f"ğŸ”„ Detected existing vector database ({CHROMA_PATH}), loading directly...")
    
    # Directly load existing database without embedding computation, very fast
    vectorstore = Chroma(
        persist_directory=CHROMA_PATH, 
        embedding_function=embeddings
    )
else:
    print(f"ğŸ†• Database not detected, creating and storing to {CHROMA_PATH}...")
    
    # Execute time-consuming embedding operations here
    vectorstore = Chroma.from_documents(
        documents=splits, 
        embedding=embeddings, 
        persist_directory=CHROMA_PATH
    )
    print("âœ… Database creation completed!")

# Set vector store as retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# ----------------- Step 4 & 5: Define RAG chain and query -----------------
# 4. Define prompt template for enhanced generation
prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ã€æ³•å¾‹æ³•è§„ä¸Šä¸‹æ–‡ã€‘æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. å›ç­”å¿…é¡»**åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡**ï¼Œä¸è¦ç¼–é€ æ³•å¾‹æ¡æ–‡ã€‚
2. å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´â€œæ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•å›ç­”â€ã€‚

ã€æ³•å¾‹æ³•è§„ä¸Šä¸‹æ–‡ã€‘:
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘:
{input}

ã€å›ç­”ã€‘:
""")






# ----------------- Run query -----------------
query = "èµ°ç§ç½ªæ€ä¹ˆåˆ¤åˆ‘ï¼Ÿ"

print(f"ğŸ” Retrieving: {query}")
retrieved_docs = retriever.invoke(query)
# Print results
print(f"\nâœ… Retrieved {len(retrieved_docs)} relevant document(s):\n")
for i, doc in enumerate(retrieved_docs):
    print(f"--- Document {i+1} ---")
    # Get content
    print(f"[Content Summary]: {doc.page_content[:100]}...") 
    # Get metadata (crime name, link, etc.)
    print(f"[Metadata]: {doc.metadata}")
    print("\n")